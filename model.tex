\chapter{Model Development}
\label{chap: model}

This chapter introduces a model development procedure used in both classification tasks in Chapter \ref{chap: rul} and regression tasks in Chapter \ref{chap: reg}. The procedure involves:
\begin{enumerate*}[label=\itshape\alph*\upshape)]
    \item signal pre-processing,
    \item feature generation,
    \item feature selection,
    \item model training,
    \item model validation, and
    \item hyperparameter tuning,
\end{enumerate*}
as shown in Figure .

\section{Signal pre-processing}
It is essential to reduce noises and extract regions of interest in signals by signal processing before we perform other analyses. Figure presents this process. First, DC bias was removed by subtracting the mean amplitude of a signal to prevent models from fitting on bias. Second, considering the computational cost from the high resolution data, we choose to downsample the ultrasonic signals. Third, we define the region or interest as the interval which contains the ultrasonic signal responses, and thus the other parts of a signal are discarded so that redundant information is not included.

\section{Feature generation}
Since ultrasonic sensor signals are unstructured, which is difficult to process, feature extraction methods are needed to create a representative set of values, i.e., features that aggregate the information from an entire signal. In this stage, physics-based and data-driven features are generated. The hybrid feature pool enables us to incorporate both physics knowledge and data-driven information into models.

\subsection{Physics-based features}
Given that physics modeling is built on theories or comprehensive experiment studies, physics-based features are robust, explainable, and suitable for applications having limited amounts of data such as the fatigue testing data in this research. Therefore, features from traditional LU and NLU testings become potential candidates for the model.
\begin{itemize}
    \item Wave velocity
    
    In LU testing, ultrasonic wave velocity is a stiffness based measure which is associated with macroscopic damage such as crack/void coalescence and propagation. The wave speed is the distance divided by the time-of-flight (TOF) that a ultrasonic wave transverses in the material, as shown by Equation \eqref{eq: wave velocity}
    
    \begin{equation}
        v = \frac{2D}{\Delta t}
        \label{eq: wave velocity}
    \end{equation}
    where wave velocity is denoted by $v$, and $D$ is the thickness of the specimen. $\Delta t$ is the time difference between the actuation pulse and the response signal. Notice that, in our LU testing setup, one transducer severs as both the transmitter and receiver. Thus, the excitation signal travels $2D$ and the phase is changed $180^{\circ} $ when received.

    \item Nonlinear acoustic parameter $\beta$
    
    While wave velocity from LU testing is able to detect fatigue damage at macro-scale, it is limited because it cannot detect defects much smaller than the probing wavelength, e.g., 1mm. In contrast, NLU techniques are based on a different physical principle: nonlinear elasticity from nano- and micro-scale defects induce harmonic generation. The nonlinear acoustic parameter is related to the amplitude of generated harmonics. This nonlinear parameter changes due to defects such as dislocations, local plastic strain, precipitates, and micro-cracks, all of which are orders of magnitude smaller than the probing wavelength. Here, we apply fast Fourier transform (FFT) to a NLU measurement and simply calculate the nonlinear parameter by using the ratio between the amplitudes of the fundamental and the harmonic waves given by Equation \eqref{eq: beta}

    \begin{equation}
        \beta = \frac{A_2}{A_1}
        \label{eq: beta}
    \end{equation}
    where $A_1$, $A_2$ is the amplitude of the fundamental wave and the second-order harmonic wave, respectively.
\end{itemize}

\subsection{Data-driven features}
The physics-based features alone, however, are not enough to capture all of the information from the LU and NLU signals. As a result, a large number of features engineered from the time domain, frequency domain, and time-frequency domain of ultrasonic measurements are added to the feature pool.

\begin{itemize}
    \item Time domain features
    
    Time domain features are peak amplitudes, ratios between peak amplitudes, and components from Principal Component Analysis (PCA) and Independent Component Analysis (ICA). Statistics in time domain such as median, quantiles, variance, skewness, and kurtosis are also included. Besides, from the envelope analysis of a NLU signal, wave duration, wave energy, and the ratios between these quantities are calculated.

    \item Frequency domain features
    
    Frequency domain analysis offers some of the information that is not presented in the time domain. This information is especially valuable for periodic signals such as ultrasonic measurements. Thus, after applying fast Fourier transform (FFT), peak amplitudes, ratios between peak amplitudes, peak frequencies, frequency centroid and variance in FFT spectrum are extracted as the frequency domain features.

    \item Time-frequency domain features
    
    Ultrasonic signals are usually not stationary, i.e., frequency changes in time, because the interaction between ultrasonic waves and discontinuities within the material. Therefore, time-frequency analysis is needed to describe the phenomena. Discrete wavelet transform (DWT) is adopted to decompose ultrasonic measurements into several frequency bands. Then, statistics such as mean, median, kurtosis, and skewness are recorded for each frequency band.
\end{itemize}

The feature pool contains XXX features in total, and a list of candidate features for LU and NLU measurements is displayed in Table

\section{Feature selection}
Feature selection aims to remove features that are redundant. Irrelevant features are common to see when we construct features without fully understanding a physical process. For example, the relationship between fatigue mechanism and ultrasonic responses. By including only the best subset of features for a prediction task, feature selection helps develop robust models against overfitting and improve model generalizability. There exists various feature selection techniques which can be mainly classified into three categories: filter methods, wrapper methods, and embedded methods. Each of these methods has its advantages, disadvantages, and suitable application scenarios.

In the model development pipeline, we adopted a wrapper method called Recursive Feature Elimination with Cross-validation (RFECV) to obtain the optimal feature subset that achieves the best predictive performance in multiple training/test data splits for a single model. Figure X shows the RFECV algorithm. First, recursive feature elimination (RFE) starts from a set with all available features and eliminate $k$ features step by step based on the feature ranking with regressors/classifiers until the predetermined number of features $n$ is reached. Nevertheless, the best number of features to select $n^*$ is not determined. To find out $n^*$ while alleviating the problem of overfitting, cross-validation (CV), a statistical model validation technique, is used along with RFE. CV partitions a dataset into training set and validation set in each fold. A model is evaluated multiple times with different partitions, and $n^*$ is determined by the overall validation results. Then, RFE selects the optimal $n^*$ features from the feature pool. We choose 5-fold classification in this feature selection procedure to avoid adding too much computation cost due to the fact that RFE is already computationally expensive.

\section{Model training and validation}
Model training and validation involve another CV loop. Notice that, however, the CV here is not for finding the best feature subset but for providing a generalized estimate of a model's performance. Specifically, leave-one-group-out CV (LOGOCV) is applied, where each group contains three repeated measurements at one measurement location in one specimen. Here, we make an assumption that each group, i.e., each measurement location in a specimen, is an independent sample because of the differences in microstructure. In LOGOCV, each group is tested once and received validation scores by a predictor trained on the other groups, which efficiently utilizes the dataset and assesses the generalization capacity of a model.

\section{Hyperparameter tuning}
Searching an optimal set of hyperparameters is another crucial stage that significantly influences model performance in ML model development. We use grid search accompanied with the validation scores from the LOGOCV to tune hyperparameter in a simple and faster manner. Grid search exhaustively considers all candidates from predefined hyperparameter combinations. The number of hyperparameters and the range of each hyperparameter vary in different learning algorithms. The detailed settings will be discussed in Chapter \ref{chap: rul} and \ref{chap: reg}.